``` r

install.packages("doBy")
#샘플링
#doBy
library(doBy)

#비복원, 임의 추출
sampleBy(~Species, data=iris, frac=0.1)

#복원 추출
sampleBy(~Specieis, data=iris, frac=0.1, replace=TRUE)

#비복원, 계통 추출
sampleBy(~Species, data=iris, frac=0.1, systematic =TRUE )


######상관 분석###############

#피어슨 상관 계수
#cor 함수 사용
#연속형, 연속형(모수적)

cor(iris[ , 1:4])

cor(iris$Petal.Length, iris$Petal.Width)
#cor.test() 함수를 사용하여 상관계수의 유의성을 검증할 수 있습니다.
cor.test(iris$Petal.Length, iris$Petal.Width)


#스피어만 상관계수
#분석하고자 하는 두 연속형 분포가 심각하게 정규분포를 벗어나는 경우(비모수적)
#혹은 두 변수가 순위 척도 자료일 때

install.packages("Hmisc")
library(Hmisc)
rcorr(iris$Petal.Width, iris$Petal.Length, type="spearman")

#독립성 검증
#단일 무작위 표본에서 추출한 자료에 대해 하나의 특성이 다른 특성에 대해 독립적인지 여부를
#알아보는 검증방법
#범주별로 빈도만이 주어진 범주형 데이터 분석은 일반적으로 카이제곱 분포를 이용한 검정법 사용
library(dplyr)
library(ggplot2)
diamonds
View(diamonds)
data <- diamonds %>% select(cut, color)
data <- table(data)
chisq.test(data)
mcnemar.test()


#다차원척도법
#평가 대상의 관계(유사성, 거리)를 활용하여 평가 대상을 n차원 공간에 표시하는 것
#근접성 자료를 공간적 거리로 시각화 가능

plot(iris[,1:4])
data<-dist(iris[,1:4], method="euclidean")
data <- cmdscale(data)

rownames(data) <- 1:150
plot(data[,1], data[,2], type="n", main="iris", xlab="x", ylab="y")
text(data[,1], data[,2], rownames(data), cex=0)

#주성분 분석법
#상관관계가 있는 기존 변수를 선형결합해 분산이 극대화된 상관관계가 없는 새로운 변수로
#축약하는 것.
data <- airquality[complete.cases(airquality),]
m <- prcomp(data[,2:5], scale=TRUE) #scale=TRUE는 정규화
m
#주성분 분석은 선형 대수에 대한 이해가 필요
#상관 계수는 벡터의 내적이다.
#선형대수에서 고유값(eigenvalue)와 고유벡터(eigenvector)
#행렬 A를 선형변환으로 봤을 때 선형변환 A에 의한 변환결과과
#자신의 상수배가 되는 0이 아닌 벡터를 고유벡터라고 함
#Ax=λx????

#간단하게 풀어보기
A = matrix(c(1,2,3,2), nrow=2, ncol=2, byrow=T)
lambda_A = eigen(A)

#손으로 계산한 것과 고유벡터 값이 다르게 보이는 이유는??

#보통 고유벡터를 표현할 때 단위 벡터로 만들어 주기 때문에
#서로 비율은 같음


#분류 모형
#의사결정 나무
install.packages("caret")
install.packages("party")
install.packages("tree")
library(caret)

library(tree)
idx <- createDataPartition(iris$Species, p=0.7)
train <- iris[idx$Resample1,]
nrow(train[train$Species =="versicolor",])
test <- iris[-idx$Resample1,]
table(train$Species)
#트리 그리기
treemod <- tree(Species~., data = train)
plot(treemod)
text(treemod)

#k-fold Cross Validation 을 이용하여 가치치기(pruning)
cv_tree<-cv.tree(treemod, FUN= prune.misclass)
plot(cv_tree)

#가지치기 이후의 그래프 표시
prune_tree <- prune.misclass(treemod, best=3)
plot(prune_tree)
text(prune_tree)

#Decision Tree의 또다른 패키지
install.packages('rpart')
library(rpart)

rpartmod<- rpart(Species~. , data=train, method="class")
plot(rpartmod)
text(rpartmod)
#
library(party)
partymod<- ctree(Species~., data=train)
plot(partymod)

#베이즈 분류
install.packages("klaR")
library(klaR)
m <- NaiveBayes(Species~., data=train) #위에 caret 패키지로 이미 쪼갠 것
op <- par(mfrow=c(2,2))
plot(m)
par(op)
pred <- predict(m)
table(pred$class, train[,5])

print(paste(round(mean(pred$class ==train[,5])*100,2),
                  "%분류 일치", sep=""))

install.packages("e1071")
library(e1071)
m <- naiveBayes(Species~., data=train)
pred <- predict(m, test[ ,-5]) #훈련데이터로 만든 예측모델에 test데이터를 넣어서 검증
table(test[,5], pred)

#KNN
library(class)
pred <- knn(train[,1:4], test[,1:4], train[,5],k=3, prob=TRUE)
table(pred, test[,5])

#ANN 인공신경망 모형
library(nnet)
m <- nnet(Species ~., data=train, size=3)

pred <- predict(m, train, type="class")
table(pred, train[,5])

pred_1 <- predict(m, test, type="class")
table(pred, test[ , 5])
```
